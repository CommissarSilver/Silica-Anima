{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import math\n",
    "from abc import ABC, abstractmethod"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Remember that each node, is a logistic regression in and out of itself\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NodeBlueprint(ABC):\n",
    "    @abstractmethod\n",
    "    def __init__(self, input_features_num:int,activation:str):\n",
    "        pass\n",
    "    @abstractmethod    \n",
    "    def forward(self, input:list):\n",
    "        pass\n",
    "    @abstractmethod\n",
    "    def backward(self, input:list, error:float, learning_rate:float):\n",
    "        pass\n",
    "    \n",
    "    @staticmethod\n",
    "    def sigmoid(x:float):\n",
    "        return 1/(1+math.exp(-x))\n",
    "    \n",
    "    @staticmethod\n",
    "    def relu(x:float):\n",
    "        return max(0,x)\n",
    "    \n",
    "    @staticmethod\n",
    "    def tanh(x:float):\n",
    "        return math.tanh(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "\n",
    "class Node:\n",
    "    def __init__(self, input_features_num: int, activation: str):\n",
    "        self.weights = [random.random() for _ in range(input_features_num)]\n",
    "        self.bias = random.random()\n",
    "        self.activation = self.sigmoid if activation == \"sigmoid\" else self.relu\n",
    "        \n",
    "        self.input_cache = []\n",
    "        self.z_cache = []\n",
    "        self.activation_cache = []\n",
    "    \n",
    "    def sigmoid(self, x):\n",
    "        return 1 / (1 + math.exp(-x))\n",
    "    \n",
    "    def relu(self, x):\n",
    "        return max(0, x)\n",
    "\n",
    "    def forward(self, layer_input: list):\n",
    "        assert len(layer_input) == len(self.weights), 'Number of weights must be equal to number of input features'\n",
    "        z = sum(i*w for i, w in zip(layer_input, self.weights)) + self.bias\n",
    "        a = self.activation(z)\n",
    "        \n",
    "        self.input_cache=layer_input.copy()\n",
    "        self.z_cache=z\n",
    "        self.activation_cache=a\n",
    "        \n",
    "        return a\n",
    "    \n",
    "    def backward(self, output_gradient: float, learning_rate: float):\n",
    "        if self.activation == self.sigmoid:\n",
    "            d_activation = self.activation_cache * (1 - self.activation_cache)\n",
    "        elif self.activation == self.relu:\n",
    "            d_activation = 1 if self.z_cache > 0 else 0\n",
    "        \n",
    "        d_z = output_gradient * d_activation\n",
    "        d_w = [d_z * i for i in self.input_cache]\n",
    "        d_b = d_z\n",
    "        \n",
    "        # Update weights and bias\n",
    "        self.weights = [w - learning_rate * dw for w, dw in zip(self.weights, d_w)]\n",
    "        self.bias -= learning_rate * d_b\n",
    "        \n",
    "        # Return gradient w.r.t. input for further backpropagation in the network\n",
    "        input_gradient = [d_z * w for w in self.weights]\n",
    "        return input_gradient\n",
    "        \n",
    "        \n",
    "        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DenseLayer():\n",
    "    def __init__(self, input_features_num:int, nodes_num:int, activation:str):\n",
    "        self.activation = activation\n",
    "        self.nodes = [Node(input_features_num,activation) for _ in range(nodes_num)]\n",
    "    \n",
    "    def forward(self, input:list):\n",
    "        return [node.forward(input) for node in self.nodes]\n",
    "    \n",
    "    def backward(self, input:list, error:list, learning_rate:float):\n",
    "        for node, err in zip(self.nodes,error):\n",
    "            node.backward(input,err,learning_rate)"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
